>Discuss at least one ‘fail’ and one ‘win’. Explicitly situate your discussion with reference to the readings and annotations by the class as a whole. Again, as I asked at the end of Modules 1 and 2, in the context of the last few weeks, what has been most challenging win/fail for you, and why is that?

My fail for module 3 is in not taking enough time for the excersizes in these final few weeks. The readings in week 11 were meant to be annotated by us students, but I had fallen behind that week and though I ended up catching up later, I felt like I potentially missed out on some discussion that could have happened because I ended up not making annotations because I knew that I was the last person in class looking at those readings. Furthermore, after peeking in the discord and seeing some of the sonofications and other projects people were able to produce, I wish I had made more time to try out more of week 12's excersizes myself.

win -->

challanging --> artistic vs rational knowledge struggle

>Given what you’ve read and explored in this last module, which was about ‘communicating’ digital archaeology, what are the implications for ‘doing’ digital archaeology for different audiences? How has your thinking evolved since the end of Module 2? Are there new dilemmas that have emerged?

### Archaeology of the Heart and Open Data

In their paper titled ["At the Heart of the Ikaahuk Archaeology Project," Lisa Hodgetts and Laura Kelvin](https://digiarch.netlify.app/data/hodgetts-kelvin.pdf) discuss a 'heart-centered approach' to archaeology:

>Through listening and trying to respond to the needs of com-munity members in Sachs Harbour, my research has become less about reconstruct-ing  past  lives  from  material  remains  and  more  about  working  with  community members  to  facilitate  their  access  to  their  archaeological  heritage  and  supporting them as they make meaning from those remains. It #ows from my personal relation-ships with community members, which makes it a work of the heart, and it is about connecting them with things that have emotional and spiritual meaning and value for  them–  things  of  the  heart.

I think it is worth considering whether we are taking a heart-centered approach towards the communities which the graveyards we recorded our data from serve. Are we ensuring that our interests align with their needs? To this question I would answer no because we are not really engaged in a dialogue with these communities, we are not keeping them updated on what we are doing with the data, and at the end of the day our use of the data is simply for our own educative enrichment. Is the best care being taken that our work will not harm these communities? Since we are using the data in quite limited ways and since our work is rarely published online, it is fairly unlikely that anything from this course will eventually be discovered by one of those communities' members by accidental chance. But we still have an obligation to those communities to not share or publish this data beyond reasonable bounds, because once data is put online it can very much take on a life of its own, and potentially fall into the hands of someone who does not take time to consider about the ethical consequences of using this data and how it may affect these communities.

It is important to consider this perspective of guarding data alongside so-called open data. The idea behind open data is that we should make data collected and used in studies available and freely accessable in order for the findings of these studies to be reproducable and therefore verifiable. In contrast to the point made above, one could argue that making data used in studies freely accessable ought to be an ethical imperative in itself, because if an argument is based upon some dataset which is unavailable, there is no way to verify the truth of it.

There is therefore a contention: do we guard data so that it does fall into the wrong hands and negatively affect the communities it is collected from or do we allow data to be freely available in order for it to be verifiable? The answer here is certainly something which is not nessesarily easy and which should certainly besided on a case-by-case basis. But most importantly, it need not be a binary decision since we can enforce varying levels of access to data. The Mukurtu heritage project is a good example of this: it is a heart-centered solution to the data sharing problem ([Hall, "Mukurtu for maatauranga Maaori: A case study in Indigenous archiving for reo and tikanga revitalisation](https://www.waikato.ac.nz/__data/assets/pdf_file/0007/394945/chapter25.pdf)).

### Learning Through Doing and Art

learning through doing (week 10, [Hodgetts and Kelvin, At the Heart of the Ikaahuk Archaeology Project](https://digiarch.netlify.app/data/hodgetts-kelvin.pdf))

creative mindset works for dissemination of theory only, we should be wary as to not fill in facts and base our models on those, the models we make are not predictative because of chaos theory, models are not the real truth and in history there are so many tiny factors which make an empire ([week 11 notes](https://github.com/Archeron1148/week-eleven/blob/main/reflection.md))

### Reproducability

Archaeological work is destructive by its very nature. When an artifact is removed from the earth it was found in it is no longer the same - an artifact can only be found one time. For this reason, the traditional scientific conception of reproducability cannot apply to archaeological phenomena, but once artifacts are abstracted into data and collected in databases, manipulations can be made on the data to bring out hiddel patterns or qualities which do not appear on the surface. Since data can be easily copied so that an original version remains undisturbed, these manipulations can be reproduced by simply following the same set of instructions on a fresh copy of the data in order to validate the results.

On a computer this reproducability is, in theory, as simple as copying the data and the code used to manipulate it, hitting 'run,' and comparing the results. In practice, its a little more complicated because of differences in machines and dependancy versioning: the same piece of python code may run differently depending on the machine and operating system it was compiled on, the version of python being used, and the versions of any libraries that have been imported. Any one of these things may have been the reason we failed in reproducing Hope Loiselle's findings.

Thankfully, there are strategies which can be used to mitagate a lot of these issues, i.e. keeping track of which versions of the dependancies were used as well as the specifications of the machine the experiment was run on. These version numbers and machine specifications along with the original data and the code to manipulate it can all be packaged together in a research compendium, which can then be used to recreate the same conditions (by using a virtual machine and downloaded the same dependacy versions) to validate the results of the experiment. [Ben Marwick et al. provide a very good article titled "Packaging Data Analytical Work Reproducibly Using R (and Friends)"](http://faculty.washington.edu/bmarwick/PDFs/Marwick-Boettiger-Mullen-2018-TAS-research-compendia.pdf) which explains the importance of using research compendiums and how to get started with creating one in R. But even so, putting together a research compendium is not a trivial task and there are still complications: we saw in week 10 how a tutorial for creating a research compendium no longer worked as intended because of some changes in its dependancies made over only a few months. 
